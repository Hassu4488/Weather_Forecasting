# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c3O_RuKNGciSE_vi5Px9NPY_LmsnRQbs
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load Data
df = pd.read_csv('/content/drive/MyDrive/capstone_project_data.csv')

df

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='Max-temp', y='mean_temp', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  from matplotlib import pyplot as plt
  import seaborn as sns
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['year']
  ys = series['mean_temp']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = df.sort_values('year', ascending=True)
for i, (series_name, series) in enumerate(df_sorted.groupby('Station')):
  _plot_series(series, series_name, i)
  fig.legend(title='Station', bbox_to_anchor=(1, 1), loc='upper left')
sns.despine(fig=fig, ax=ax)
plt.xlabel('year')
_ = plt.ylabel('mean_temp')

from matplotlib import pyplot as plt
df['Max-temp'].plot(kind='hist', bins=20, title='Max-temp')
plt.gca().spines[['top', 'right',]].set_visible(False)

df.describe()

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  from matplotlib import pyplot as plt
  import seaborn as sns
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['year']
  ys = series['Min-temp']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = _df_8.sort_values('year', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('year')
_ = plt.ylabel('Min-temp')

df.info()

# Convert 'year' and 'month' to a datetime object named 'Date'
df['Date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'], format='%Y-%b')
df = df.drop(['year', 'month'], axis=1)

df

df.info()

# Scale features for each station
features_to_scale = ['Min-temp', 'Max-temp', 'mean_temp', 'Precipitation', 'Humidiy']
# Get unique stations from the original DataFrame before one-hot encoding
unique_stations = df['Station'].unique()

# One-hot encode the 'Station' column
df_encoded = pd.get_dummies(df, columns=['Station'], prefix=['Station'])

# Dictionary to store scaler objects for each station and feature
scalers = {}

for station in unique_stations:
    mask = df_encoded['Station_' + station] == 1
    scalers[station] = {}
    for feature in features_to_scale:
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(df_encoded.loc[mask, [feature]])
        df_encoded.loc[mask, feature] = scaled_data.ravel()  # Flatten to 1D array
        scalers[station][feature] = scaler


    # Use the mask to select the rows for the current station and the columns to scale
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(df_encoded.loc[mask, features_to_scale])

    # Now use .loc with the mask and the list of columns to assign the scaled data back to the DataFrame
    df_encoded.loc[mask, features_to_scale] = scaled_data

# Train-Test Split based on 'Date'
train_mask = (df_encoded['Date'] >= '1984-01-01') & (df_encoded['Date'] <= '2004-12-31')
test_mask = (df_encoded['Date'] >= '2005-01-01') & (df_encoded['Date'] <= '2014-12-31')

# Include the one-hot encoded 'Station' columns for training and testing data
train_data_with_stations = df_encoded[train_mask].drop(['Date'], axis=1)
test_data_with_stations = df_encoded[test_mask].drop(['Date'], axis=1)

train_data_with_stations

test_data_with_stations.head(10)

# Function to create sequences
def create_sequences(data, sequence_length):
    sequences = []
    targets = []
    for i in range(len(data) - sequence_length):
        sequences.append(data.iloc[i:i+sequence_length].values)
        targets.append(data.iloc[i+sequence_length].values)
    return np.array(sequences), np.array(targets)

# Create sequences
sequence_length = 24
X_train_with_stations, y_train_with_stations = create_sequences(train_data_with_stations, sequence_length)
X_test_with_stations, y_test_with_stations = create_sequences(test_data_with_stations, sequence_length)

df.columns

X_test_with_stations

# Define the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X_train_with_stations.shape[1], X_train_with_stations.shape[2])))
model.add(LSTM(50))
model.add(Dense(y_train_with_stations.shape[1]))  # The output layer has neurons corresponding to the features

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
history = model.fit(X_train_with_stations, y_train_with_stations, epochs=100, batch_size=32, validation_split=0.1, verbose=1)

# Evaluate the model
loss = model.evaluate(X_test_with_stations, y_test_with_stations, verbose=1)
print(f'Test loss: {loss}')

# Make predictions
predictions = model.predict(X_test_with_stations)

# Specify the path where you want to save the model
model_save_path = '/content/drive/MyDrive/capstone prj/finalized.h5'

# Save the model
model.save(model_save_path)

print(f'Model saved to: {model_save_path}')

# Plot the training and validation loss curves
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Curves')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

from datetime import timedelta

def make_future_predictions_per_station(model, test_data, n_steps, scalers, feature_names, unique_stations):
    future_predictions = {station: [] for station in unique_stations}

    # Extract the last sequence for each station
    last_sequences = {}
    for station in unique_stations:
        last_sequence = test_data[test_data['Station_' + station] == 1].values[-sequence_length:]
        last_sequences[station] = last_sequence

    # Predict future for each station
    for station in unique_stations:
        current_sequence = last_sequences[station].copy()
        for _ in range(n_steps):
            next_step = model.predict(current_sequence[np.newaxis, :, :])
            future_predictions[station].append(next_step[0])
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1] = next_step[0]

    # Inverse transform and organize predictions
    organized_predictions = []
    for station in unique_stations:
        predictions = np.array(future_predictions[station])
        for i, feature in enumerate(feature_names):
            feature_column = predictions[:, i]
            station_scaler = scalers[station][feature]
            predictions[:, i] = station_scaler.inverse_transform(feature_column.reshape(-1, 1)).flatten()
        # Add station predictions to organized list
        organized_predictions.append((station, predictions))

    return organized_predictions

# Future dates for predictions
last_date = df_encoded['Date'].max()
future_dates = [last_date + timedelta(days=30 * i) for i in range(1, 10 * 12 + 1)]  # Monthly data for 10 years

# Number of steps for 10 years prediction
n_steps = 10 * 12  # Number of months in 10 years

# Predict the future for each station
future_predictions = make_future_predictions_per_station(
    model, test_data_with_stations, n_steps, scalers, features_to_scale, unique_stations
)

# Adjust the column names to include both features and one-hot encoded station columns
all_column_names = features_to_scale + ['Station_' + station for station in unique_stations]

# Create DataFrames for each station
future_dfs = {}
for station, predictions in future_predictions:
    # Ensure the DataFrame has the correct number of columns
    future_dfs[station] = pd.DataFrame(predictions, columns=all_column_names, index=future_dates)

# Print or save the future predictions DataFrame for each station
for station, df in future_dfs.items():
    print(f"Future Predictions for {station} Station")
    print(df.head())

import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

# Evaluate the model
y_true = y_test_with_stations  # Actual values
y_pred = model.predict(X_test_with_stations)  # Predicted values

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_true, y_pred)
print(f'Mean Squared Error (MSE): {mse}')

# Calculate R-squared (R2 score)
r2 = r2_score(y_true, y_pred)
print(f'R-squared (R2 score): {r2}')

# Flatten the 3D arrays to 2D for plotting
y_true_flat = y_true.reshape(-1, y_true.shape[-1])
y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])

# Plot actual vs. predicted values for the first station and feature
plt.plot(y_true_flat[:, 0], label='Actual', marker='o')
plt.plot(y_pred_flat[:, 0], label='Predicted', marker='o')

plt.title(f'Actual vs. Predicted for {features_to_scale[0]} at {unique_stations[0]}')
plt.xlabel('Time Steps')
plt.ylabel(features_to_scale[0])
plt.legend()
plt.show()

import os

# Directory where the CSV files will be saved
save_dir = "/content/predictions"

# Create the directory if it does not exist
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# Save each station's predictions as a CSV file
for station, df in future_dfs.items():
    # Construct the file path
    file_path = os.path.join(save_dir, f"{station}_future_predictions.csv")

    # Save the DataFrame to a CSV file
    df.to_csv(file_path)

    print(f"Saved: {file_path}")